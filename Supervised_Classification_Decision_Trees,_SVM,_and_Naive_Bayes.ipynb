{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9YIuP7q/4JxPkXVfGJwmy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hem-githu/python/blob/main/Supervised_Classification_Decision_Trees%2C_SVM%2C_and_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoYQFHphlKuC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Classification: Decision Trees, SVM, and Naive Bayes|Assignment\n",
        "\n",
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Ans: Information Gain is a measure of how much a feature reduces uncertainty (entropy) in a dataset. In Decision Trees, it is used to decide which feature to split on at each step, ensuring the tree grows in a way that maximizes classification accuracy.\n",
        "\n",
        "\n",
        "What is Information Gain?\n",
        "- Definition: Information Gain (IG) quantifies the reduction in entropy (randomness or impurity) when a dataset is split based on a particular feature.\n",
        "- Entropy: A measure of disorder or impurity in the dataset. For classification, entropy is highest when classes are evenly mixed and lowest when all samples belong to one class.\n",
        "\n",
        "How It Is Used in Decision Trees\n",
        "- Step 1: Calculate Entropy of the dataset\n",
        "Example: If 50% samples are \"Yes\" and 50% are \"No,\" entropy is maximum (1 bit).\n",
        "- Step 2: Split on a feature\n",
        "Each feature divides the dataset into subsets.\n",
        "- Step 3: Compute Information Gain\n",
        "The reduction in entropy after the split is the IG.\n",
        "- Step 4: Choose the feature with highest IG\n",
        "The tree algorithm (like ID3, C4.5) selects the feature that maximizes IG for the next node.\n",
        "- Step 5: Repeat recursively\n",
        "Continue splitting until stopping criteria are met (e.g., pure nodes, max depth).\n",
        "\n",
        " Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        " Ans: Gini Impurity and Entropy are both measures of impurity used in decision trees, but they differ in how they calculate uncertainty.\n",
        "- Entropy comes from information theory. It measures the average amount of “information” or surprise in the dataset.\n",
        "- If the classes are evenly split (say 50% Yes, 50% No), entropy is at its maximum because the uncertainty is highest. If all samples belong to one class, entropy is zero because there’s no uncertainty.\n",
        "- Like entropy, Gini is zero when the node is pure (all samples in one class). It reaches its maximum when classes are evenly distributed.\n",
        "\n",
        "The practical difference is that entropy uses logarithms, which makes it more theoretically grounded in information theory, while Gini relies on squared probabilities, making it computationally faster. In practice, both often lead to very similar splits in decision trees.\n",
        "Intuitively:\n",
        "- Entropy tells you how much “information” is needed to classify an observation.\n",
        "- Gini tells you how often you’d be wrong if you guessed based on the distribution.\n",
        "In usage:\n",
        "- CART (Classification and Regression Trees) typically use Gini.\n",
        "- ID3 and C4.5 algorithms often use Entropy.\n",
        "\n",
        "\n",
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Ans:Pre-pruning in decision trees is a technique used to stop the tree from growing too deep or complex by applying constraints during the construction process, rather than waiting until the tree is fully grown.\n",
        "\n",
        " What is Pre-Pruning?\n",
        "- When building a decision tree, the algorithm keeps splitting nodes to reduce impurity.\n",
        "- If left unchecked, the tree can grow very large, perfectly fitting the training data but performing poorly on unseen data (overfitting).\n",
        "- Pre-pruning prevents this by setting rules that stop further splitting early.\n",
        "\n",
        " Common Pre-Pruning Strategies\n",
        "- Maximum Depth: Limit how many levels the tree can grow.\n",
        "- Minimum Samples per Split: Require a minimum number of samples before a node can be split.\n",
        "- Minimum Information Gain / Gini Reduction: Only split if the improvement in purity is above a threshold.\n",
        "- Maximum Number of Leaf Nodes: Restrict the total number of terminal nodes.\n",
        "\n",
        " Use of Pre-Pruning:\n",
        "- Avoid Overfitting: Keeps the tree simpler and more generalizable.\n",
        "- Reduce Complexity: Smaller trees are easier to interpret.\n",
        "- Improve Efficiency: Saves computation time by avoiding unnecessary splits\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "onWOPLNblSZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Train a Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Display results in a neat format\n",
        "importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"Feature Importances (using Gini Impurity):\")\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N03fiQe-nAvS",
        "outputId": "9438a516-56c8-4b77-a9fe-e4e8102d068e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances (using Gini Impurity):\n",
            "             Feature  Importance\n",
            "2  petal length (cm)    0.564056\n",
            "3   petal width (cm)    0.422611\n",
            "0  sepal length (cm)    0.013333\n",
            "1   sepal width (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Ans: A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and sometimes regression tasks. Its main idea is to find the best boundary (called a hyperplane) that separates data points of different classes with the maximum margin.\n",
        "\n",
        " Core Concept\n",
        "- Imagine you have two classes of points on a graph.\n",
        "- SVM tries to draw a line (in 2D) or a hyperplane (in higher dimensions) that separates the classes.\n",
        "- The best hyperplane is the one that maximizes the distance (margin) between itself and the nearest points from each class.\n",
        "- These nearest points are called support vectors, and they are critical because they define the boundary\n",
        "\n",
        "Working:\n",
        "- Linear SVM: Finds a straight hyperplane to separate classes if data is linearly separable.\n",
        "- Non-linear SVM: Uses a technique called the kernel trick to project data into a higher-dimensional space where a linear separation is possible.\n",
        "- Common kernels: Linear, Polynomial, Radial Basis Function (RBF), Sigmoid.\n",
        "- Margin Maximization: Ensures the classifier is robust and generalizes well to unseen data.\n",
        "\n",
        "\n",
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Ans:The Kernel Trick in Support Vector Machines (SVM) is a clever mathematical technique that allows SVMs to handle data that is not linearly separable by implicitly mapping it into a higher-dimensional space — without ever computing that mapping directly.\n",
        "\n",
        "The Problem\n",
        "- A simple SVM works well when data can be separated by a straight line (or hyperplane).\n",
        "- But many real-world datasets are non-linear — you can’t separate them with a straight boundary.\n",
        "\n",
        "The Kernel Trick\n",
        "- Instead of explicitly transforming data into a higher dimension (which could be computationally expensive), SVM uses a kernel function.\n",
        "- A kernel computes the similarity between two data points in the higher-dimensional space, without actually performing the transformation.\n",
        "- This makes it possible to find complex, non-linear boundaries efficiently\n",
        "\n",
        "Common Kernel Functions\n",
        "- Linear Kernel: Works when data is linearly separable.\n",
        "- Polynomial Kernel: Captures curved boundaries by considering polynomial combinations of features.\n",
        "- Radial Basis Function (RBF) Kernel: Popular choice; creates circular or radial decision boundaries.\n",
        "- Sigmoid Kernel: Similar to neural networks’ activation functions.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yw6l1zt7nOXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel=\"linear\", random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel=\"rbf\", random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracy with Linear Kernel:\", accuracy_linear)\n",
        "print(\"Accuracy with RBF Kernel:\", accuracy_rbf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79kcndlCnw5I",
        "outputId": "72656aa2-52f9-46ca-d439-d9129194ac34"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear Kernel: 0.9444444444444444\n",
            "Accuracy with RBF Kernel: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "\n",
        "Ans: The Naïve Bayes classifier is a simple yet powerful probabilistic machine learning algorithm based on Bayes’ Theorem. It is widely used for classification tasks such as spam detection, sentiment analysis, and text categorization.\n",
        "\n",
        "What is Naïve Bayes?\n",
        "- It applies Bayes’ Theorem to calculate the probability that a given data point belongs to a particular class.\n",
        "\n",
        "Why  \"Naïve\"?\n",
        "- The algorithm makes a naïve assumption: it assumes that all features are independent of each other given the class.\n",
        "- In reality, features often have correlations (e.g., in text classification, the words “solar” and “panel” are related).\n",
        "- Despite this unrealistic assumption, Naïve Bayes often performs surprisingly well in practice, especially for high-dimensional data like text.\n",
        "\n",
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes?\n",
        "\n",
        "Ans: Gaussian Naïve Bayes\n",
        "- Data type: Continuous (real-valued) features.\n",
        "- Assumption: Each feature follows a normal (Gaussian) distribution within each class.\n",
        "- Use case: Works well for datasets like sensor readings, exam scores, or continuous measurements (e.g., height, weight, solar irradiance values).\n",
        "- Example: Classifying whether a patient has a disease based on continuous lab test results.\n",
        "\n",
        "Multinomial Naïve Bayes\n",
        "- Data type: Discrete counts or frequency data.\n",
        "- Assumption: Features represent counts (non-negative integers), often word frequencies in text.\n",
        "- Use case: Common in text classification (spam detection, sentiment analysis) where features are word counts or TF-IDF values.\n",
        "- Example: Classifying emails as spam or not spam based on word occurrence counts.\n",
        "\n",
        "Bernoulli Naïve Bayes\n",
        "- Data type: Binary features (0 or 1).\n",
        "- Assumption: Each feature is a yes/no indicator (present or absent).\n",
        "- Use case: Useful when features represent presence/absence rather than counts.\n",
        "- Example: Classifying documents based on whether certain keywords appear at least once (not how many times).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Sr00GR8n8_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset:\", accuracy)"
      ],
      "metadata": {
        "id": "QIEJLonfnLeT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}